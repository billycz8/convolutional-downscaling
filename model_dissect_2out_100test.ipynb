{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#%%\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grad_loss(v_gt, v):\n",
    "    # Gradient loss\n",
    "    loss = tf.reduce_mean(tf.abs(v - v_gt), axis=[1,2,3])\n",
    "    jy = v[:,:,1:,:,:] - v[:,:,:-1,:,:]\n",
    "    jx = v[:,:,:,1:,:] - v[:,:,:,:-1,:]\n",
    "    jy_ = v_gt[:,:,1:,:,:] - v_gt[:,:,:-1,:,:]\n",
    "    jx_ = v_gt[:,:,:,1:,:] - v_gt[:,:,:,:-1,:]\n",
    "    loss += tf.reduce_mean(tf.abs(jy - jy_), axis=[1,2,3])\n",
    "    loss += tf.reduce_mean(tf.abs(jx - jx_), axis=[1,2,3])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uNet(input, time, lat, lon, height, kernel = [5, 3, 3], nodes = [72, 144, 288, 576]):\n",
    "    '''\n",
    "    This function defines a U-Net architecture\n",
    "    :param input: the main-input layer\n",
    "    :param time: the time input layer\n",
    "    :param lat, lon, height: additional fields\n",
    "    :param kernel: Kernel-sizes (default = [5, 3, 3])\n",
    "    :param nodes: different neuron-sizes if needed (default = [72, 144, 288, 576])\n",
    "    :return: last layer of constructed model\n",
    "    '''\n",
    "\n",
    "    # set Timesteps\n",
    "    TS = 3\n",
    "    ##################################################### 1st Block ####################################################\n",
    "    conv1 = Conv3D(filters      = nodes[0],\n",
    "                   kernel_size  = (TS, kernel[0], kernel[0]),\n",
    "                   activation   = 'relu',\n",
    "                   padding      = 'same',\n",
    "                   data_format  = 'channels_last')(input)\n",
    "    mergetime = Concatenate(axis=4)([conv1, lat, lon, height])\n",
    "    conv1 = Conv3D(filters      = nodes[0],\n",
    "                   kernel_size  = kernel[0],\n",
    "                   activation   = 'relu',\n",
    "                   padding      = 'same',\n",
    "                   data_format  = 'channels_last')(mergetime)\n",
    "\n",
    "    pool1 = MaxPooling3D(pool_size = (1, 2, 2))(conv1)\n",
    "\n",
    "    ##################################################### 2nd Block ####################################################\n",
    "    conv2 = Conv3D(filters      = nodes[1],\n",
    "                   kernel_size  = (TS, kernel[1], kernel[1]),\n",
    "                   activation   = 'relu',\n",
    "                   padding      = 'same',\n",
    "                   data_format  = 'channels_last')(pool1)\n",
    "    conv2 = Conv3D(filters      = nodes[1],\n",
    "                   kernel_size  = (TS, kernel[1], kernel[1]),\n",
    "                   activation   = 'relu',\n",
    "                   padding      = 'same',\n",
    "                   data_format  = 'channels_last')(conv2)\n",
    "\n",
    "    pool2 = MaxPooling3D(pool_size = (1, 2, 2))(conv2)\n",
    "\n",
    "    ##################################################### 3rd Block ####################################################\n",
    "    conv3 = Conv3D(filters      = nodes[2],\n",
    "                   kernel_size  = (TS, kernel[2], kernel[2]),\n",
    "                   activation = 'relu',\n",
    "                   padding      = 'same',\n",
    "                   data_format  = 'channels_last')(pool2)\n",
    "    conv3 = Conv3D(filters      = nodes[2],\n",
    "                   kernel_size  = (TS, kernel[2], kernel[2]),\n",
    "                   activation='relu',\n",
    "                   padding      = 'same',\n",
    "                   data_format  = 'channels_last')(conv3)\n",
    "\n",
    "    pool3 = MaxPooling3D(pool_size = (1, 2, 2))(conv3)\n",
    "\n",
    "    ##################################################### 4th Block ####################################################\n",
    "    conv4 = Conv3D(filters      = nodes[3],\n",
    "                   kernel_size  = (TS, kernel[2], kernel[2]),\n",
    "                   activation='relu',\n",
    "                   padding      = 'same',\n",
    "                   data_format  = 'channels_last')(pool3)\n",
    "    conv4 = Conv3D(filters      = nodes[3],\n",
    "                   kernel_size  = (TS, kernel[2], kernel[2]),\n",
    "                   activation='relu',\n",
    "                   padding      = 'same',\n",
    "                   data_format  = 'channels_last')(conv4)\n",
    "\n",
    "    ####################################################### TIME #######################################################\n",
    "    # Merge time-layer at this point\n",
    "    mergetime = Concatenate(axis=4)([conv4, time])\n",
    "\n",
    "    ################################################### UP 3rd Block ###################################################\n",
    "    # Up-Size again\n",
    "    up3   = UpSampling3D(size = (1, 2, 2))(mergetime)\n",
    "    up3   = Conv3D(filters              = nodes[2],\n",
    "                   kernel_size          = (TS, kernel[1], kernel[1]),\n",
    "                   activation           = 'relu',\n",
    "                   padding              = 'same',\n",
    "                   kernel_initializer   = 'he_normal')(up3)\n",
    "\n",
    "    # Skip connection\n",
    "    merge3 = Concatenate(axis=4)([conv3, up3])\n",
    "\n",
    "    conv3 = Conv3D(filters              = nodes[2],\n",
    "                   kernel_size          = (TS, kernel[1], kernel[1]),\n",
    "                   activation           = 'relu',\n",
    "                   padding              = 'same',\n",
    "                   data_format          = 'channels_last')(merge3)\n",
    "    conv3 = Conv3D(filters              = nodes[2],\n",
    "                   kernel_size          = (TS, kernel[1], kernel[1]),\n",
    "                   activation           = 'relu',\n",
    "                   padding              = 'same',\n",
    "                   data_format          = 'channels_last')(conv3)\n",
    "\n",
    "    ################################################### UP 2nd Block ###################################################\n",
    "    up2 = UpSampling3D(size = (1, 2, 2))(conv3)\n",
    "    up2 = Conv3D(filters                = nodes[1],\n",
    "                 kernel_size            = (TS, kernel[1], kernel[1]),\n",
    "                 activation             = 'relu',\n",
    "                 padding                = 'same',\n",
    "                 kernel_initializer     = 'he_normal')(up2)\n",
    "\n",
    "    # Skip connection\n",
    "    merge2 = Concatenate(axis=4)([conv2, up2])\n",
    "\n",
    "    conv2 = Conv3D(filters              = nodes[1],\n",
    "                   kernel_size          = (TS, kernel[1], kernel[1]),\n",
    "                   activation           = 'relu',\n",
    "                   padding              = 'same',\n",
    "                   data_format          = 'channels_last')(merge2)\n",
    "    conv2 = Conv3D(filters              = nodes[1],\n",
    "                   kernel_size          = (TS, kernel[1], kernel[1]),\n",
    "                   activation           = 'relu',\n",
    "                   padding              = 'same',\n",
    "                   data_format          = 'channels_last')(conv2)\n",
    "\n",
    "    ################################################### UP 1st Block ###################################################\n",
    "    up1 = UpSampling3D(size = (1, 2, 2))(conv2)\n",
    "    up1 = Conv3D(filters                = nodes[0],\n",
    "                 kernel_size            = (TS, kernel[0], kernel[0]),\n",
    "                 activation             = 'relu',\n",
    "                 padding                = 'same',\n",
    "                 kernel_initializer     = 'he_normal')(up1)\n",
    "\n",
    "    merge1 = Concatenate(axis=4)([conv1, up1])\n",
    "\n",
    "    conv1 = Conv3D(filters              = nodes[0],\n",
    "                   kernel_size          = (TS, kernel[0], kernel[0]),\n",
    "                   activation           = 'relu',\n",
    "                   padding              = 'same',\n",
    "                   data_format          = 'channels_last')(merge1)\n",
    "    conv1 = Conv3D(filters              = nodes[0],\n",
    "                   kernel_size          = (TS, kernel[0], kernel[0]),\n",
    "                   activation           = 'relu',\n",
    "                   padding              = 'same',\n",
    "                   data_format          = 'channels_last')(conv1)\n",
    "\n",
    "    # last layer is the output\n",
    "    output = conv1\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(PS=32, loss = grad_loss, optimizer = 'adam', nodes = [72, 144, 288, 576], residual = False):\n",
    "    '''\n",
    "    This function creates the DCN-architecture (residual = False) or RPN-architecture (residual = True).\n",
    "    :param PS: Patch size\n",
    "    :param loss: loss function (default = grad_loss)\n",
    "    :param optimizer: optimizer (default = 'adam')\n",
    "    :param nodes: different neuron-sizes if needed (default = [72, 144, 288, 576])\n",
    "    :param residual: boolean toggeling between RPN (True) and DCN (False)\n",
    "    :return: Model\n",
    "    '''\n",
    "\n",
    "    # Input layers\n",
    "    main_input  = Input(shape = (3, PS, PS, 1))\n",
    "    time        = Input(shape = (3, int(PS/8), int(PS/8), 1))\n",
    "    lat         = Input(shape = (3, PS, PS, 1))\n",
    "    lon         = Input(shape = (3, PS, PS, 1)) \n",
    "    height      = Input(shape = (3, PS, PS, 1))\n",
    "\n",
    "    # Load U-Net\n",
    "    unet        = uNet(main_input, time, lat, lon, height, nodes = nodes)\n",
    "\n",
    "    # Define output layer after U-Net\n",
    "    temp_out    = Conv3D(filters        = 1,\n",
    "                         kernel_size    = (3, 1, 1),\n",
    "                         activation     = 'linear',\n",
    "                         padding        = 'same',\n",
    "                         data_format    = \"channels_last\")(unet)\n",
    "\n",
    "    def transpose_fn(x):\n",
    "      return tf.transpose(x, perm=[0, 4, 2, 3, 1])\n",
    "    temp_out = Lambda(transpose_fn)(temp_out)\n",
    "    # # Define output layer after U-Net\n",
    "    temp_out    = Conv3D(filters        = 2,\n",
    "                         kernel_size    = (1, 1, 1),\n",
    "                         activation     = 'linear',\n",
    "                         padding        = 'valid',\n",
    "                         data_format    = \"channels_last\")(temp_out)\n",
    "    temp_out = Lambda(transpose_fn)(temp_out)\n",
    "    \n",
    "    # residual layer\n",
    "    if residual:\n",
    "        temp_out = Add()([main_input[:,1,:,:], temp_out])\n",
    "\n",
    "    # create model with the defined Layers\n",
    "    model       = Model(inputs          = [main_input, time, lat, lon, height],\n",
    "                        outputs         = temp_out)\n",
    "\n",
    "    # compile with defined loss and optimizer\n",
    "    model.compile(loss      = loss,\n",
    "                  optimizer = optimizer,\n",
    "                  metrics   = ['mse', 'mae', 'mape'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "  model = get_model() # DCN\n",
    "#   model = get_model(residual=True) # RPN\n",
    "  model.summary()\n",
    "  print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want data in the size of patch_size x patch_size in time step of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52704, 23, 34, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "wind_speed_data = np.load('tensor.npy')  # Assuming wind speed is in the first channel\n",
    "print(wind_speed_data.shape)\n",
    "wind_speed_data = wind_speed_data[:, :, :, 0]  # Extract wind speed channel\n",
    "\n",
    "# Normalize the data\n",
    "wind_speed_data = (wind_speed_data - np.min(wind_speed_data)) / (np.max(wind_speed_data) - np.min(wind_speed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52704, 23, 34)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_speed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input patches shape: (52700, 3, 32, 32, 1)\n",
      "Input time patches shape: (52700, 3, 4, 4, 1)\n",
      "Label patches shape: (52700, 2, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reshape data to fit the model input shape\n",
    "def reshape_data(data, target_shape=(32, 32)):\n",
    "    # Reshape data to the target shape\n",
    "    data = data[..., np.newaxis]\n",
    "    resized_data = tf.image.resize(data, target_shape, method='bilinear')\n",
    "    data = resized_data.numpy()\n",
    "    return data\n",
    "\n",
    "def create_patches(data, time_steps):\n",
    "    num_samples = data.shape[0] - time_steps + 1\n",
    "    training_patches = []\n",
    "    patches = []\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(num_samples):\n",
    "        patch = data[i:i+time_steps]\n",
    "        patches.append(patch[1::2])\n",
    "        training_patches.append(patch[::2])\n",
    "    training_timestep = time_steps//2+1\n",
    "    relative_time = np.zeros((time_steps//2+1, data.shape[1]//8, data.shape[2]//8, 1))\n",
    "    # relative_time = np.zeros((3, 4, 4, 1))\n",
    "\n",
    "    # Assign specific values to each slice along the first axis\n",
    "    # relative_time[0, :, :, 0] = -1\n",
    "    # relative_time[1, :, :, 0] = 0\n",
    "    # relative_time[2, :, :, 0] = 1\n",
    "    for t in range(training_timestep):\n",
    "        relative_time[t, :, :, 0] = t*2\n",
    "    relative_time_patches = np.tile(relative_time, (num_samples, 1, 1, 1, 1))\n",
    "    training_patches = np.array(training_patches)\n",
    "    patches = np.array(patches)\n",
    "\n",
    "    return training_patches, relative_time_patches, patches\n",
    "\n",
    "\n",
    "# Parameters\n",
    "patch_size = 32\n",
    "time_steps_label = 5\n",
    "\n",
    "# Reshape data\n",
    "reshaped_data = reshape_data(wind_speed_data, target_shape=(32, 32))\n",
    "\n",
    "# Create label patches with time_steps_label\n",
    "input_patches, input_time_patches, label_patches = create_patches(reshaped_data, time_steps_label)\n",
    "\n",
    "# input_patches, input_time_patches = input_patches[:label_patches.shape[0],:,:,:,:], input_time_patches[:label_patches.shape[0],:,:,:,:]\n",
    "\n",
    "print(\"Input patches shape:\", input_patches.shape)\n",
    "print(\"Input time patches shape:\", input_time_patches.shape)\n",
    "print(\"Label patches shape:\", label_patches.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_patches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get one month\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m one_month_size \u001b[38;5;241m=\u001b[39m \u001b[43minput_patches\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m12\u001b[39m\n\u001b[1;32m      3\u001b[0m input_patches \u001b[38;5;241m=\u001b[39m input_patches[:one_month_size]\n\u001b[1;32m      4\u001b[0m input_time_patches \u001b[38;5;241m=\u001b[39m input_time_patches[:one_month_size]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_patches' is not defined"
     ]
    }
   ],
   "source": [
    "# get one month\n",
    "one_month_size = input_patches.shape[0]//12\n",
    "input_patches = input_patches[:one_month_size]\n",
    "input_time_patches = input_time_patches[:one_month_size]\n",
    "label_patches = label_patches[:one_month_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data\n",
    "# train_inputs, val_inputs, train_inputs_time, val_inputs_time = train_test_split(\n",
    "#     input_patches, input_time_patches, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_labels, val_labels, train_labels_time, val_labels_time = train_test_split(\n",
    "#     label_patches, test_size=0.2, random_state=42)\n",
    "# print(\"Training input patches shape:\", train_inputs.shape)\n",
    "# print(\"Training input time patches shape:\", train_inputs_time.shape)\n",
    "# print(\"Val input time patches shape:\", val_inputs_time.shape)\n",
    "# print(\"Validation input patches shape:\", val_inputs.shape)\n",
    "# print(\"Training label patches shape:\", train_labels.shape)\n",
    "# print(\"Validation label patches shape:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the training set\n",
    "train_size = int(0.85 * len(input_patches))  # 80% for training\n",
    "\n",
    "# Create indices\n",
    "indices = np.arange(len(input_patches))\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split indices into training and validation\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "# Use the indices to split the data\n",
    "train_inputs = input_patches[train_indices]\n",
    "val_inputs = input_patches[val_indices]\n",
    "\n",
    "train_inputs_time = input_time_patches[train_indices]\n",
    "val_inputs_time = input_time_patches[val_indices]\n",
    "\n",
    "train_labels = label_patches[train_indices]\n",
    "val_labels = label_patches[val_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input patches shape: (3732, 3, 32, 32, 1)\n",
      "Training input time patches shape: (3732, 3, 4, 4, 1)\n",
      "Val input time patches shape: (659, 3, 4, 4, 1)\n",
      "Validation input patches shape: (659, 3, 32, 32, 1)\n",
      "Training label patches shape: (3732, 2, 32, 32, 1)\n",
      "Validation label patches shape: (659, 2, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training input patches shape:\", train_inputs.shape)\n",
    "print(\"Training input time patches shape:\", train_inputs_time.shape)\n",
    "print(\"Val input time patches shape:\", val_inputs_time.shape)\n",
    "print(\"Validation input patches shape:\", val_inputs.shape)\n",
    "print(\"Training label patches shape:\", train_labels.shape)\n",
    "print(\"Validation label patches shape:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs = val_inputs[:10,:,:,:,:]\n",
    "\n",
    "val_inputs_time = val_inputs_time[:10,:,:,:,:]\n",
    "\n",
    "val_labels = val_labels[:10,:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3732, 2, 32, 32, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26s/step - loss: 1.8453 - mae: 1.6264 - mape: 600.1868 - mse: 81.8075 \n",
      "Epoch 1: val_loss improved from inf to 0.01509, saving model to best_model.keras\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3047s\u001b[0m 26s/step - loss: 1.8335 - mae: 1.6159 - mape: 596.3328 - mse: 81.2744 - val_loss: 0.0151 - val_mae: 0.0122 - val_mape: 3.5255 - val_mse: 2.4802e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0168 - mae: 0.0143 - mape: 4.5482 - mse: 3.5405e-04 \n",
      "Epoch 2: val_loss improved from 0.01509 to 0.00660, saving model to best_model.keras\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2084s\u001b[0m 18s/step - loss: 0.0168 - mae: 0.0142 - mape: 4.5429 - mse: 3.5332e-04 - val_loss: 0.0066 - val_mae: 0.0042 - val_mape: 1.5884 - val_mse: 3.4209e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0212 - mae: 0.0186 - mape: 5.7420 - mse: 7.0684e-04 \n",
      "Epoch 3: val_loss did not improve from 0.00660\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2053s\u001b[0m 18s/step - loss: 0.0212 - mae: 0.0186 - mape: 5.7440 - mse: 7.0729e-04 - val_loss: 0.0502 - val_mae: 0.0467 - val_mape: 12.5774 - val_mse: 0.0027\n",
      "Epoch 4/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0224 - mae: 0.0199 - mape: 6.2385 - mse: 7.6762e-04 \n",
      "Epoch 4: val_loss did not improve from 0.00660\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2052s\u001b[0m 18s/step - loss: 0.0224 - mae: 0.0199 - mape: 6.2236 - mse: 7.6433e-04 - val_loss: 0.0111 - val_mae: 0.0090 - val_mape: 2.5301 - val_mse: 1.2603e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0193 - mae: 0.0170 - mape: 5.3008 - mse: 4.9579e-04 \n",
      "Epoch 5: val_loss did not improve from 0.00660\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2054s\u001b[0m 18s/step - loss: 0.0194 - mae: 0.0170 - mape: 5.3087 - mse: 4.9749e-04 - val_loss: 0.0088 - val_mae: 0.0063 - val_mape: 2.3905 - val_mse: 6.7017e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0108 - mae: 0.0086 - mape: 2.8751 - mse: 1.2883e-04 \n",
      "Epoch 6: val_loss did not improve from 0.00660\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2066s\u001b[0m 18s/step - loss: 0.0108 - mae: 0.0086 - mape: 2.8747 - mse: 1.2882e-04 - val_loss: 0.0070 - val_mae: 0.0046 - val_mape: 1.7523 - val_mse: 3.7212e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0105 - mae: 0.0084 - mape: 2.7903 - mse: 1.2094e-04 \n",
      "Epoch 7: val_loss did not improve from 0.00660\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2059s\u001b[0m 18s/step - loss: 0.0105 - mae: 0.0084 - mape: 2.7908 - mse: 1.2101e-04 - val_loss: 0.0068 - val_mae: 0.0046 - val_mape: 1.5926 - val_mse: 3.5727e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0098 - mae: 0.0077 - mape: 2.5581 - mse: 1.3339e-04 \n",
      "Epoch 8: val_loss did not improve from 0.00660\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2064s\u001b[0m 18s/step - loss: 0.0099 - mae: 0.0077 - mape: 2.5724 - mse: 1.3586e-04 - val_loss: 0.0223 - val_mae: 0.0190 - val_mape: 4.4951 - val_mse: 5.8730e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0232 - mae: 0.0205 - mape: 6.3294 - mse: 7.0536e-04 \n",
      "Epoch 9: val_loss did not improve from 0.00660\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2064s\u001b[0m 18s/step - loss: 0.0231 - mae: 0.0204 - mape: 6.3189 - mse: 7.0342e-04 - val_loss: 0.0192 - val_mae: 0.0171 - val_mape: 4.3441 - val_mse: 3.9362e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0128 - mae: 0.0106 - mape: 3.3830 - mse: 1.9325e-04 \n",
      "Epoch 10: val_loss did not improve from 0.00660\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2058s\u001b[0m 18s/step - loss: 0.0128 - mae: 0.0106 - mape: 3.3809 - mse: 1.9303e-04 - val_loss: 0.0127 - val_mae: 0.0103 - val_mape: 2.8469 - val_mse: 1.4941e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0138 - mae: 0.0115 - mape: 3.6454 - mse: 2.3754e-04 \n",
      "Epoch 11: val_loss did not improve from 0.00660\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2061s\u001b[0m 18s/step - loss: 0.0138 - mae: 0.0115 - mape: 3.6441 - mse: 2.3740e-04 - val_loss: 0.0143 - val_mae: 0.0122 - val_mape: 3.3683 - val_mse: 2.3087e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0129 - mae: 0.0107 - mape: 3.3902 - mse: 1.8867e-04 \n",
      "Epoch 12: val_loss improved from 0.00660 to 0.00566, saving model to best_model.keras\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2058s\u001b[0m 18s/step - loss: 0.0129 - mae: 0.0107 - mape: 3.3890 - mse: 1.8857e-04 - val_loss: 0.0057 - val_mae: 0.0036 - val_mape: 1.2645 - val_mse: 2.3500e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0077 - mae: 0.0057 - mape: 1.9374 - mse: 6.4859e-05 \n",
      "Epoch 13: val_loss did not improve from 0.00566\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2058s\u001b[0m 18s/step - loss: 0.0078 - mae: 0.0057 - mape: 1.9404 - mse: 6.5183e-05 - val_loss: 0.0062 - val_mae: 0.0040 - val_mape: 1.3498 - val_mse: 2.8460e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - loss: 0.0080 - mae: 0.0059 - mape: 2.0140 - mse: 7.0213e-05 \n",
      "Epoch 14: val_loss did not improve from 0.00566\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2058s\u001b[0m 18s/step - loss: 0.0080 - mae: 0.0059 - mape: 2.0216 - mse: 7.1145e-05 - val_loss: 0.0123 - val_mae: 0.0101 - val_mape: 2.5531 - val_mse: 1.5772e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47s/step - loss: 0.0122 - mae: 0.0099 - mape: 3.1875 - mse: 1.6720e-04 \n",
      "Epoch 15: val_loss did not improve from 0.00566\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5476s\u001b[0m 47s/step - loss: 0.0122 - mae: 0.0099 - mape: 3.1876 - mse: 1.6722e-04 - val_loss: 0.0157 - val_mae: 0.0135 - val_mape: 3.4019 - val_mse: 2.6520e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - loss: 0.0105 - mae: 0.0084 - mape: 2.7377 - mse: 1.1751e-04 \n",
      "Epoch 16: val_loss did not improve from 0.00566\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2500s\u001b[0m 21s/step - loss: 0.0105 - mae: 0.0084 - mape: 2.7382 - mse: 1.1761e-04 - val_loss: 0.0186 - val_mae: 0.0163 - val_mape: 4.1721 - val_mse: 4.1108e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - loss: 0.0117 - mae: 0.0096 - mape: 3.0588 - mse: 1.5708e-04 \n",
      "Epoch 17: val_loss did not improve from 0.00566\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2489s\u001b[0m 21s/step - loss: 0.0117 - mae: 0.0096 - mape: 3.0584 - mse: 1.5710e-04 - val_loss: 0.0071 - val_mae: 0.0049 - val_mape: 1.4105 - val_mse: 3.7476e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22s/step - loss: 0.0082 - mae: 0.0061 - mape: 2.0423 - mse: 7.0657e-05 \n",
      "Epoch 18: val_loss did not improve from 0.00566\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2559s\u001b[0m 22s/step - loss: 0.0082 - mae: 0.0061 - mape: 2.0430 - mse: 7.0724e-05 - val_loss: 0.0186 - val_mae: 0.0164 - val_mape: 3.9150 - val_mse: 3.9617e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m 96/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7:35\u001b[0m 22s/step - loss: 0.0147 - mae: 0.0126 - mape: 3.8871 - mse: 2.8874e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     10\u001b[0m     filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Filepath where the model will be saved\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,        \u001b[38;5;66;03m# Monitor the validation loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m                  \u001b[38;5;66;03m# Verbosity mode\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model with validation and checkpointing\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_inputs_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_inputs_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/mcgill/reproducibility/venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the model\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "model = get_model(PS=batch_size)\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model.keras',  # Filepath where the model will be saved\n",
    "    monitor='val_loss',        # Monitor the validation loss\n",
    "    save_best_only=True,       # Save only the best model\n",
    "    save_weights_only=False,   # Save the whole model, not just weights\n",
    "    mode='min',                # Mode 'min' means it will save the model with the minimum validation loss\n",
    "    verbose=1                  # Verbosity mode\n",
    ")\n",
    "\n",
    "# Train the model with validation and checkpointing\n",
    "history = model.fit(\n",
    "    [train_inputs, train_inputs_time, train_inputs, train_inputs, train_inputs],\n",
    "    train_labels,\n",
    "    validation_data=([val_inputs, val_inputs_time, val_inputs, val_inputs, val_inputs], val_labels),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = input_patches[-test_size:,:,:,:,:]\n",
    "\n",
    "test_inputs_time = input_time_patches[-test_size:,:,:,:,:]\n",
    "\n",
    "test_labels = label_patches[-test_size:,:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_fn(x):\n",
    "      return tf.transpose(x, perm=[0, 4, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "best_model = load_model('best_model.keras',custom_objects={'grad_loss': grad_loss, \"transpose_fn\": transpose_fn})\n",
    "\n",
    "# Now you can use `best_model` to make predictions or further evaluations\n",
    "predictions = best_model.predict([test_inputs,test_inputs_time,test_inputs,test_inputs,test_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# # Load the saved model\n",
    "# best_model = load_model('best_model.keras',custom_objects={'grad_loss': grad_loss, \"transpose_fn\": transpose_fn})\n",
    "\n",
    "# # Now you can use `best_model` to make predictions or further evaluations\n",
    "# predictions = best_model.predict([val_inputs,val_inputs_time,val_inputs,val_inputs,val_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-807.3833"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((test_labels[:,:,:,:,:] - predictions[:,:,:,:,:])/ test_labels[:,:,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2, 32, 32, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean interpolation\n",
    "interpolation = [(test_inputs[:,1:2,:,:,:] + test_inputs[:,0:1,:,:,:]) / 2, (test_inputs[:,2:3,:,:,:] + test_inputs[:,1:2,:,:,:]) / 2]\n",
    "interpolation_result = np.concatenate(interpolation, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011145579"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs((test_labels - predictions)/ test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020976929"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs((test_labels - interpolation) / test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import zoom\n",
    "def trilinear_downscale(inputs, target_size):\n",
    "    \"\"\"\n",
    "    Downscales the second dimension (dimension 1) using trilinear interpolation.\n",
    "    \n",
    "    Args:\n",
    "        inputs (np.ndarray): Input array of shape (N, D, H, W, C)\n",
    "        target_size (int): The target size for the second dimension (D)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Downscaled array of shape (N, new_D, H, W, C)\n",
    "    \"\"\"\n",
    "    # Compute the zoom factor for downscaling\n",
    "    zoom_factor = target_size / inputs.shape[1]\n",
    "    \n",
    "    # Create a zoom array where the zoom factor is applied to dimension 1\n",
    "    zoom_factors = [1.0, zoom_factor, 1.0, 1.0, 1.0]\n",
    "    \n",
    "    # Apply the zoom function\n",
    "    downscaled = zoom(inputs, zoom_factors, order=1)  # order=1 for trilinear interpolation\n",
    "    \n",
    "    return downscaled\n",
    "target_size = 2  # New size for the dimension 1\n",
    "\n",
    "downscaled_interpolation = trilinear_downscale(test_inputs, target_size)\n",
    "print(downscaled_interpolation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.238095e-05 6.520077e-05 3.6313635e-05\n"
     ]
    }
   ],
   "source": [
    "mse_prediction = np.mean((test_labels - predictions) ** 2)\n",
    "mse_baseline = np.mean((test_labels - interpolation) ** 2)\n",
    "mse_interpolation = np.mean((test_labels - downscaled_interpolation) ** 2)\n",
    "print(mse_prediction, mse_baseline, mse_interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
